---
title: "Class Notes"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

#Class 1: 09/12/19

##Built in Functions
-There are special commands in R that allow you to solve simple math problems 
-You can find the sum, mean, 

```{r, eval=FALSE}
x <-c (1,2,3)

sum(x)
length(x) #length of vector 
var(x) #variance of vector 
range(x) #returns highest value and lowest value
sd(x)
sqrt(x)

sort(x) #automatically sorts values in increasing order. 
sort(x, decreasing=TRUE) #sorts values in decreasing order 

sort(x)[1] #Returns the first number in the values sorted by small to big AKA the smallest number!

unique(x) #Returns the unique (different) elements of x
table(x) #Returns the frequencies of x
class(x) #Returns the type of value that x is 


```

-You can create your own functions 

```{r, eval=FALSE}
name_of_function <- function(){
  
  return(result)
  
#Simply but the input in the () next to function, and tell what you want to return in the () of return
}


```
 
-Example of writing a simple function
```{r, eval=FALSE}

sum_of_numbers <- function(y){
  total <- 0
  for (x in 1:y){
    total <- total + x
  }
  return(total)
}
```

##Dealing with values in a table 
###Creating a matrix

```{r, eval=FALSE}
my_matrix <- matrix(data=seq(from=1, to=100), nrow=20, ncol=5) #Creates a matrix with the numbers from 1 to 100, with 20 rows and 5 columns 


my_matrix["row","column"]
my_matrix[1,] #Shows you everything in row 1
my_matrix[,1] #Shows you everything in column 1

my_matrix[c(1,5),] #shows you all of the values in row 1 and 5


```

###Indexing Values in a Matrix

```{r, eval=FALSE}
c1to20<- 1:20 

my_matrix[c1to20<10,]

df <-data.frame(my_matrix)


```


###Installing data packages 
```{r, eval=FALSE}
install.packages("babynames")
library(babynames)

babynames[babynames$year == 2017 & babynames$sex == "F", ]
```


#Class 3: Packages 
```{r, eval=FALSE}
install.packages('dplyr')
install.packages('ggplot')

library(dplyr)
library(babynames)

```

##Selecting columns 
```{r, eval=FALSE}
select(babynames,year,name) #Using the select function
babynames[c('year', 'name')] #Using indexing


```


##Filter Function
```{r, eval=FALSE}
babynames %>%
  filter(year==1950 & sex=='F') %>%
  select(name)

#In dplyr you can use "%>%" to indicate that you are performing that function within some data set
#%>% indicates that you are taking the output of the previous statement, and using it as the input for the following function
#This function allows you to write the code in a shorter, but understandable way instead of writing a function like this: select(filter(babynames, year > 1950), name)

```


##Summarize dataset
```{r, eval=FALSE}

babynames %>%
  summarise(first_year = min(year),
            last_year = max(year),
            mean_N = mean(n),
            std_N = sd(n),
            N = length(n))

#You are assigning values to a certain name but you are using "=" instead of "<-"

```

##Group By 
```{r, eval=FALSE}
babynames %>%
  group_by(sex)
  
```

##Combining these dply functions 
```{r, eval=FALSE}
babynames %>%
  filter(year==1950 & sex=='F') %>%
  summarise(first_year = min(year),
            last_year = max(year),
            mean_N = mean(n),
            std_N = sd(n),
            N = length(n))
```


##Using ggplot2
```{r, eval=FALSE}

install.packages('gapminder') #gapminder is a useful dataset
install.packages('ggplot2')
library(gapminder)
library(ggplot2)

data <- gapminder

```

```{r, eval=FALSE}
lifeexp_by_year <- data %>%
  group_by(year) %>% 
  summarise(avg_life = mean(lifeExp))

#Summarise should be last! Final output of what you have already filtered or selected 


```


```{r, eval=FALSE}
#First command is always "ggplot" 


ggplot(lifeexp_by_year, aes(x=year, y=avg_life)) + #Must add "+" at the end of every separate line
  geom_point() + #makes a scatter point
  geom_line() #makes a line graph. ALSO has points because of above command

#aes is the command for the aesthetic. You can change the way the graph looks by adding to this function


```

##Making a bar graph 
```{r, eval=FALSE}
lifeexp_by_continent <- data %>%
  group_by(continent) %>% 
  summarise(avg_life = mean(lifeExp))


ggplot(lifeexp_by_continent, aes(x=continent, 
                                      y=avg_life,
                                      fill=continent)) + 
  geom_bar(stat = 'identity') #Must say "stat='identity'"

#Changing the type of graph will often mean you want to change the order of the aes, depending on what you want to look 
#You also want to change "color" to "fill" so the colors are solid 


```


##Making a line graph
```{r, eval=FALSE}

lifeexp_by_continent <- data %>%
  group_by(continent) %>% 
  summarise(avg_life = mean(lifeExp))


ggplot(lifeexp_by_year_continent, aes(x=year, 
                                      y=avg_life,
                                      color=continent)) + 
  geom_point() + 
  geom_line() +
#You can also change the general look of the plot by changing the "theme"
  theme_minimal()



```


##Making a histgram
```{r, eval=FALSE}
ggplot(data, aes(x=lifeExp)) +
      geom_histogram(bins=20) +
      ggtitle("Av") +
      xlab("Years") +
      ylab("Number of People") 


```

#Class 4: 09.26.19 

##Dplyr vs. Other methods 
```{r, eval=FALSE}
library(babynames)
library(dplyr)
```

```{r, eval=FALSE}
#Mutate: Used when you want to change columns or if you want to add a column

data <- babynames

#If you want to change your proportion to percentages 

babynames$prop *100 #This will not change the values in the columns but will give you percentage. 

#Use mutate to add values to the data itself 

percentage_data <- data %>%
  mutate(prop_percentage=prop*100)

```



```{r, eval=FALSE}
data_name <- data %>%
  arrange(name) #allows you to change the order of the names. This is alphabetial order starting at a


data_year <- data %>% #using "desc" allows your to have it from smaller to larger 
  arrange(desc(n))


data_most_pop_2010 <- data %>%
  filter(year > 2000 & year < 2010) %>%
  group_by(year, sex) %>%
  summarize(max_n=max(n),
      top_name=name[n==max_n]) 
  
  
  
unique_names <- data %>%
  group_by(year,sex) %>%
  summarize(unique_n = length(unique(name)))

ggplot(unique_names, aes(x=year, y=unique_n, color=sex)) +
  geom_line(size=1.5)

  
```



##If/else statement
```{r, eval=FALSE}

ifelse(1>0, 'Hey', 'Bye') #Thing to evaluate, "if true", "if false"

data %>%
  mutate(male=ifelse(sex=='M', 1, 0),
         female=ifelse(sex=='F', 1,0)) %>%
  group_by(name) %>%
  summarize(male_total_n=sum(male),
            female_total_n=sum(female),
            count=sum(n)) %>%
  filter(male_total_n > 3000 & female_total_n > 3000) %>%
  group_by()
  


```



#Class 4: Last ggplot and dplyr class 

##Using the mpg dataset and ggplot2
```{r, eval=FALSE}
library(ggplot2)
?mpg

ggplot(data=mpg, aes(x=displ, y=hwy, color=class))+ 
    #This will have two types of information. Adding "color" gives more info
  geom_point


ggplot(data=mpg, aes(x=displ, y=hwy, shape=cyl))+ 
    #You can also have the key be formatted according to different "size", "shape"
  geom_point()
  
  
ggplot(data=mpg, aes(x=displ, y=hwy))+ 
  geom_point()+
  facet_wrap(~class) #This command makes a different plot for every class of car, or whatever else you want!


ggplot(data=mpg, aes(x=displ, y=hwy))+ 
  geom_point()+
  geom_smooth(method='lm') #This gives you a line of best fit! You can 


ggplot(data=mpg, aes(x=displ, y=hwy))+ 
  geom_point(aes(color=class))+ #Defining color here allows you to have colored points (and a key), but an        overall line of best fit 
  geom_smooth(method='lm', color=black) 


ggplot(data=mpg, aes(x=class, y=hwy))+ 
  geom_boxplot(aes(fill=class)) #A boxplot with colors and filled (instead of "color", which isn't filled)


ggplot(data=mpg, aes(x=class, y=hwy))+ 
  geom_boxplot(aes(fill=class))+
  coord_flip() #This flips the x and y

```


##Using the "NYCflights13" and dplyr
```{r, eval=FALSE}
install.packages('nycflights13')
library(nycflights13)
library(dplyr)


flights %>%
  select(day, month)

#Finding the most delayed flight
flights %>%
  arrange(desc(dep_delay))


flights %>%
  select(year:day)

#You can exclude certain columns in this way: 
flights %>%
  select(-c(year,day))

#Put the arrival time first, the dep_delay second and then everything else after 
flights %>%
  select(arr_time, dep_delay, everything())


#Putting the time in hours and minutes 
flights %>%
  mutate(hour=dep_time %/% 100,
         minutes=dep_time %/% 100) %>%
  select(dep_time, hour, minutes)


#When are flights the most delayed? 
flights %>%
  summarize(mean_delay=mean(dep_delay))
 #This won't run because there are a number of NA's in the dep_delay (canceled flights)

#This is how we delete the NA values. This gives you a value because you have taken out the NA values 
mean(flights$dep_delay, na.rm=TRUE)


#When are flights the most delayed? 
flights %>%
  group_by(month) %>%
  summarize(mean_delay=mean(dep_delay, na.rm=TRUE)) 
  

#Are international flights more delayed than domestic? 
dist_vs_delay <- flights %>%
  group_by(dest) %>%
  summarize(count=n(),
            distance=mean(distance, na.rm = TRUE),
            delay =mean(arr_delay, na.rm = TRUE)) %>%
  filter(count > 20 & dest!='HNL')

#Graphing the above 
ggplot(dist_vs_delay, aes(x=distance, y=delay, size=count)) +
  geom_point()+
  geom_smooth(method='lm')
      

```



##Most canceled flights in what month?
```{r, eval=FALSE}

canceled <- flights %>% 
  group_by(month) %>%
  summarize(count_cancelled=sum(is.na(dep_delay)))



ggplot(canceled, aes(x=month, y=count_cancelled))+
  geom_line()

#Converts month to a category (factor), and ensures that when you are graphing you don't have partial month numbers 

```


#October 10: Probability and binomial distributions 

##Experimentally creating the binomial distributions
```{r, eval=FALSE}
numbers <- c(1:10)

sample(numbers, 5, replace=TRUE) 
  #tells what you are sampling from, and how many times, and whether or   not that are replacing 

sampled_numbers <- sample(numbers,10000,replace = TRUE)

hist(sampled_numbers)
  #The more you sample, the closer to uniform it becomes 

```

##Creating a random, uniform distribution
```{r, eval=FALSE}
runif(10, min = 0, max = 1)
  #sampled 10 numbers, with a range of 0 to 1

```

##The binomial distribution
```{r, eval=FALSE}
binomialresults <- rbinom(5,1, 0.5)
  #5=number of flips, 1=number of coins, 0.5=probaility of getting heads

mean(binomialresults)
  #the more times you repeat it (the larger the x) the closer you will get to 0.5

```

##Simulating the binomial distribution: flipping 10 coins and find the mean, over and over
```{r, eval=FALSE}

flip_means <- c()
n_experiments <- 100000
for(i in 1:n_experiments) {
  flip_means[i] <- mean(rbinom(10,1,0.5))
}

hist(flip_means) 

```


##Simulating the normal distribution: 
```{r, eval=FALSE}
rnorm(n=5, mean=10, sd=4)
  #Give us 5 numbers with a mean 10, sd 4 in the shape of a normal        distribution

mean(rnorm(5,10,4))

hist(rnorm(50,10,4))


flip_meansnorm <- c()
n_experiments <- 100000
for(i in 1:n_experiments) {
  flip_meansnorm[i] <- mean(rnorm(5,20,5))
}

mean(flip_meansnorm)

sd(flip_meansnorm)
  #This is the SEM! Because it is the sd of the many flips 

```

##Scores in an experiment 
```{r, eval=FALSE}
scores <- rnorm(10*2000,0,1)
  #200 numbers, with mean=0, sd=1
samples <-rep(1:10, each=20)
  #samples of your experiment 
my_df <- data.frame(samples,scores)

library(ggplot2)
library(dplyr)

ggplot(my_df, aes(x=scores)) +
  geom_histogram()+
  facet_wrap(~samples) +
  theme_classic()


sample_means <- my_df %>%
  group_by(samples) %>%
  summarise(means=mean(scores))

```

##Using pnorm: tells you the percentage of values below your mean
```{r, eval=FALSE}
pnorm(0,mean=0,sd=1)
pnorm(1,mean=0, sd=1)

```

##Using qnorm: 
```{r, eval=FALSE}
qnorm(0.025)
  #first number is area and you can get the corresponding z-score
```

```{r, eval=FALSE}
?gamma

```


#October 17: Distributions (continued)

##Calculate the proability of getting one head from two coin flips
```{r, eval=FALSE}
dbinom(x=1, size = 2, prob=0.5)
  #x=number of heads you want to observe, size=two coins/flops,           prob=fairness of coin aka in one flip what do you expect?
dbinom(x=1, size = 3, prob=0.5)
  #one head with three coins? 
```

##Addition Rule: A or B is the P(A) + P(B)
```{r, eval=FALSE}
#On an exam, what is the probability of getting of getting 4 or 5 questions right? There are 4 choices per question and 16 total questions

dbinom(x=4, size = 16, prob=0.25) + dbinom(x=5, size = 16, prob=0.25)

#What is the probaility of getting at least 4 questions right? 
1-(dbinom(x=0, size = 16, prob=0.25) + dbinom(x=1, size = 16, prob=0.25) + dbinom(x=2, size = 16, prob=0.25) + dbinom(x=3, size = 16, prob=0.25))

```

##Another way to find if you want at least 4 questions right 
```{r}
pbinom(q=3, size = 16, prob = 0.25, lower.tail = FALSE)
  #q=size of questions you DON'T get right, this funciton automatically   takes the lower tail, so if you want upper tail, you must specify that

```
##The normal distribution
```{r}
rbinom(n=10, size=2, prob = 0.5)
  #calls staring with "r" will give you a random number 
  #n=number of coins you have 
  #size=number of flips that you have 
  #This will spit out the number of heads that you get 

```

##Getting information on the number of coins 
```{r}
b_samples <- rbinom(n=10000, size=10, prob = 0.5)
hist(b_samples)
table(b_samples)/sum(b_samples)
  #this gives you the probability of 0 heads, 1 head and 2 heads in this   sample. This prob. will change sample to sample. 
```


#Simulating the normal distribution
```{r}
x<- seq(-4,4, length=100)
hx <- dnorm(x)
my_df <- data.frame(x, hx)
library(ggplot2)
ggplot(data = my_df, aes(x=x, y=hx)) +
  geom_line()

#This gives a point (likelihood)
dnorm(x=-2, mean=0, sd-1)

```

#Calulating area under the curve of the normal distribution
```{r}

pnorm(q=0, mean=0, sd=1, lower.tail = TRUE)
  #q=where do you want to start calculating area, on x

```

##Calculating the area between two points on normal distribution
```{r}
#Area between 1 and -1
pnorm(q=1, mean=0, sd=1, lower.tail = TRUE) - pnorm(q=-1, mean=0, sd=1, lower.tail = TRUE)
```

##Random number generator on the normal curve
```{r}
rnorm(n=10, mean=0, sd=1)
  #generates 10 numbers from the normal distribution, with mean=0 and     sd=1

```

##Simulating the area under the curve 
```{r}

numbers <- rnorm(n=1000000, mean=0, sd=1)
mean(numbers > -1 & numbers < 1)


```


##Problem: what is the percentage of people with IQ less than 125?
```{r}
#Normally distributed, mean=100, sd=15
#There are two ways to do this, mathimatically using the normal curve and then by sampling 100 people and average their IQs. 

#Mathimatically: 
pnorm(q=125, mean=100, sd=15, lower.tail = TRUE)

#Simulating it: 
IQnumbers <- rnorm(n=10000000, mean = 100, sd=15)
mean(IQnumbers < 125)
  #the greater the sample size, the closer you get to the mathematical    way

```

##Problem: what is the percentage of people with IQ greater than 125?
```{r}
pnorm(q=125, mean=100, sd=15, lower.tail = FALSE)
```

##Using percentiles in the normal distribution
```{r}
qnorm(p=0.15, mean=0, sd=1, lower.tail = TRUE)
  #give a probaility and get value. 

pnorm(q=-1, mean=0, sd=1, lower.tail = TRUE)
  #in pnorm you give a value and get a   probability 

```


##Sampling: you are generally not able to know the population values. Must sample to estimate. N vs N-1 in sd
```{r}
iq_small_sample <-rnorm(n=5, mean=100, sd=15)
hist(iq_small_sample)
mean(iq_small_sample)

#divide by N
sqrt(sum((iq_small_sample-mean(iq_small_sample))^2) / (length(iq_small_sample)))

#divide by N-1
sd(iq_small_sample)

#Simulating why we use N vs N-1
n_exp <- 1000
n_samples <- c(1:10)
means_of_samples <- c()
sds_of_samples_uncorrected <- c()

for (this_samples_n in n_samples) {
  this_samples_mean <- c()
  this_samples_sd <- c()
  for (i in 1:n_exp) {
    iq_sample <- rnorm(n=this_samples_n, mean=100, sd=15)
    this_samples_mean[i] <- mean(iq_sample)
    this_samples_sd [1] <-sqrt(sum((iq_small_sample-mean(iq_small_sample))^2) / (length(iq_small_sample)))
  }
  means_of_samples[this_samples_n] <- mean(this_samples_mean)
  sds_of_samples_uncorrected[this_samples_n] <- mean(this_samples_sd)
}

result <- data.frame (n=n_samples, 
                      means=means_of_samples, 
                      sds=sds_of_samples_uncorrected)


library(ggplot2)
ggplot(results, aes(x=n, y=means)) +
  geom_line(color='red') +
  geom_point() +
  geom_hline(yintercept = 100) +
  ylim(c(95, 105))

ggplot(results, aes(x=n, y=sds))+
  geom_line(color='red') +
  geom_point()+
  geom_hline(yintercept = 15) +
  ylim(c(-1,17))

```


#October 31: Hypothesis testing 

##You see a 6% increase in performance after the intervention: is this due to chance? Here we simulate the probability that it is only due to chance
```{r}
mean_differnces <- c() 
sample_size <- 100
n_exps <- 1000 
pop_mean <- 70
pop_sd <- 20

for (i in 1:n_exps){
group_experimental <- rnorm(n=sample_size, mean=pop_mean, sd=pop_sd) 
group_control <- rnorm(n=sample_size, mean=pop_mean, sd=pop_sd)
mean_differnces[i] <- mean(group_experimental) - mean(group_control)
}

hist(mean_differnces, breaks=20)


pnorm(q=6, mean=mean(mean_differnces), sd = sd(mean_differnces), lower.tail = FALSE) + pnorm(q=-6, mean=mean(mean_differnces), sd = sd(mean_differnces), lower.tail = TRUE)

```


##The randomization test 
```{r}
mean_differnces <- c() 
sample_size <- 20
n_exps <- 1000 
pop_mean <- 70
pop_sd <- 20

group_exp <- rnorm(n=sample_size, mean=pop_mean, sd=pop_sd) 
group_ctl <- rnorm(n=sample_size, mean=pop_mean, sd=pop_sd)
group_both <- c(group_exp, group_ctl)
group_assignments <- rep(c("a", "b"), each=sample_size)


sample(group_assignments) #shuffles the group assignments 

for (i in 1:n_exps) {
  shuffled_grp_assignments <- sample(group_assignments)
  group_exp_shuffled <- group_both[shuffled_grp_assignments=="a"]
  group_ctl_shuffled <- group_both[shuffled_grp_assignments=="b"]
  mean_differnces[i] <- mean(group_exp_shuffled) -mean(group_ctl)
}

hist(mean_differnces, breaks = 20)

quantile(mean_differnces, c(0.025, 0.975))

```

##Randomization test with uniform
```{r}
mean_differnces <- c() 
sample_size <- 20
n_exps <- 1000 
pop_mean <- 70
pop_sd <- 20

group_exp <- runif(n=sample_size, min=50, max=100) 
group_ctl <- runif(n=sample_size, min=50, max=100) 
group_both <- c(group_exp, group_ctl)
group_assignments <- rep(c("a", "b"), each=sample_size)


sample(group_assignments) #shuffles the group assignments 

for (i in 1:n_exps) {
  shuffled_grp_assignments <- sample(group_assignments)
  group_exp_shuffled <- group_both[shuffled_grp_assignments=="a"]
  group_ctl_shuffled <- group_both[shuffled_grp_assignments=="b"]
  mean_differnces[i] <- mean(group_exp_shuffled) -mean(group_ctl)
}

hist(mean_differnces, breaks = 20)

quantile(mean_differnces, c(0.025, 0.975))

```


#November 7th: Statistical Power
-Power: the ability of the experiment to detect an effect that is actually there. 
-Type I: you say there is an effect when there is no effect 
-Type II: you say there is no effect when there is an effect 
-Increasing alpha (type I), increases your statistical power. 
```{r}

sample_size <- 20
effect_size <- 10
pop_mean <- 70
pop_sd <- 20
alpha <- 0.05
n_runs <- 100
significance <- c()

for (run in 1:n_runs) {
  grp_experimental <- rnorm(n=sample_size, mean=pop_mean+effect_size, sd=pop_sd)
  grp_control <- rnorm(n=sample_size, mean=pop_mean, sd=pop_sd)
  pvalue <- t.test(x=grp_experimental, y=grp_control)$p.value
  significance[run] <- pvalue 
}

mean(significance < alpha)

```

##Statistical power varying with sample size 
```{r}
power_vs_sample_size <- c()
sample_size <- seq(2,200,10)

for (sample_size_ind in 1:length(sample_size)){
  this_sample_size <- sample_size[sample_size_ind]
  effect_size <- 10
  pop_mean <- 70
  pop_sd <- 20
  alpha <- 0.05
  n_runs <- 100
  significance <- c()
  for (run in 1:n_runs) {
    grp_experimental <- rnorm(n=this_sample_size, mean=pop_mean+effect_size, sd=pop_sd)
    grp_control <-      rnorm(n=this_sample_size, mean=pop_mean, sd=pop_sd)
    pvalue <- t.test(x=grp_experimental, y=grp_control)$p.value
    significance[run] <- pvalue 
  }
power_vs_sample_size[sample_size_ind] <- mean(significance < alpha)
}

library(ggplot2)
my_df <- data.frame(n=sample_size, power=power_vs_sample_size)

ggplot(my_df, aes(x=n, y=power))+
  geom_line()+
  geom_point()


```


##You are limited by an N or 20. Only option is ti manipulate effect size/variance
```{r}
power_vs_effect_size <- c()
effect_size <- seq(1,50,1)

for (effect_size_ind in 1:length(effect_size)) {
  this_effect_size <- effect_size[effect_size_ind]
  sample_size <- 20
  pop_mean <- 70
  pop_sd <- 20
  alpha <- 0.05
  n_runs <- 100
  significance <- c()
  for (run in 1:n_runs) {
    grp_experimental <- rnorm(n=sample_size, mean=pop_mean+this_effect_size, sd=pop_sd)
    grp_control <- rnorm(n=sample_size, mean=pop_mean, sd=pop_sd)
    pvalue <- t.test(x=grp_experimental, y=grp_control)$p.value
    significance[run] <- pvalue 
  }
power_vs_effect_size[effect_size_ind] <- mean(significance < alpha)
}

library(ggplot2)
my_df <- data.frame(effect_size=effect_size, power=power_vs_effect_size)

ggplot(my_df, aes(x=effect_size, y=power))+
  geom_line()+
  geom_point()

```

#November 14th: t-tests

##One sample t-test: does my group come from a given population mean? 
```{r}
pop_mean <- 70
sample_mean <- 75
sample_sd <- 20
sample_size <- 20
sample_scores <- rnorm(n=sample_size, mean=sample_mean, sd= sample_sd)

t.test(sample_scores, mu=pop_mean)


measure_of_effect <- mean(sample_scores) - pop_mean
measure_of_error <- sd(sample_scores) / sqrt(sample_size)
measure_of_effect / measure_of_error

```

##Independent t-test
```{r}
sample_1_scores <- rnorm(n=sample_size, mean=sample_mean, sd=sample_sd)
sample_2_scores <- rnorm(n=sample_size, mean=sample_mean+10, sd=sample_sd)

t.test(sample_1_scores, sample_2_scores, var.equal = TRUE)

```

##Dependent/paired t-test
```{r}
sample_1_scores_t1 <- rnorm(n=sample_size, mean=sample_mean, sd=sample_sd)
sample_1_scores_t2 <- rnorm(n=sample_size, mean=sample_mean+10, sd=sample_sd)

t.test(sample_1_scores, sample_2_scores, var.equal = TRUE, paired = TRUE)

#Pulling out a value from the t-test result 

result <- t.test(sample_1_scores, sample_2_scores, var.equal = TRUE, paired = TRUE)

result$p.value

```


##Simulating the t-distribution
```{r}
ts <- c()
sample_size <- 10
for (i in 1:1000){
  ts[i] <- t.test(rnorm(n=sample_size),
                 rnorm(n=sample_size))$statistic
}

library(ggplot2)

ggplot(data.frame(values=ts), aes(x=values))+
  geom_density()

mean(ts < -2 | ts > 2)

df_vector <- c(3,1000)

ts <- c()
n_sims <- 1000
for (d in 1:length(df_vector)){
  df <- df_vector[d]
  sample_size <- df + 1
  for (i in 1:n_sims){
    ts <- c(ts, t.test(rnorm(n=sample_size),
                       rnorm(n=sample_size))$statistic)
  }
}

my_df <- data.frame(values=ts,
                    dof=as.factor(rep(df_vector, each=n_sims)))
ggplot(my_df, aes(x=values, color=dof))+
  geom_density(size=2)



```


##Simulating how standard deviation changes with t statistic 
```{r}


library(ggplot2)
my_df <- data.frame(values=ts_vs_sd,
                    sd=sample_sd_vector)

ggplot()
```


##Simulating how sample size changes with t statistic 
```{r}
ts_vs_n <- c()
pop_mean <- 70
sample_size_vector <- 2:100
sample_mean <- 74
sample_sd <- 20
n_exps <- 100
sample_sd_vector <- 2:20
for (ind in 1:length(sample_size_vector)){
  sample_size <- sample_size_vector[ind]
  
}

```

